# LLM-security-testing-dataset
A comprehensive dataset for Large Language Model (LLM) security evaluation, featuring three categories: Benign, Borderline, and Malicious. This repository provides critical data support for AI safety research, red teaming, and the development of robust model defense strategies
